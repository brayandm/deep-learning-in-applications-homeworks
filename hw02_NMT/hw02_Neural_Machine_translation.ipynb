{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab assignment 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Machine Translation in the wild\n",
    "In the third homework you are supposed to get the best translation you can for the EN-RU translation task.\n",
    "\n",
    "Basic approach using RNNs as encoder and decoder is implemented for you. \n",
    "\n",
    "Your ultimate task is to use the techniques we've covered, e.g.\n",
    "\n",
    "* Optimization enhancements (e.g. learning rate decay)\n",
    "\n",
    "* Transformer/CNN/<whatever you select> encoder (with or without positional encoding)\n",
    "\n",
    "* attention/self-attention mechanism\n",
    "\n",
    "* pretraining the language models (for decoder and encoder)\n",
    "\n",
    "* or just fine-tunning BART/ELECTRA/... ;)\n",
    "\n",
    "to improve the translation quality. \n",
    "\n",
    "__Please use at least three different approaches/models and compare them (translation quality/complexity/training and evaluation time).__\n",
    "\n",
    "Write down some summary on your experiments and illustrate it with convergence plots/metrics and your thoughts. Just like you would approach a real problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline solution BLEU score is quite low. Try to achieve at least __21__ BLEU on the test set. \n",
    "The checkpoints are:\n",
    "\n",
    "* __21__ - minimal score to submit the homework, 30% of points\n",
    "\n",
    "* __25__ - good score, 70% of points\n",
    "\n",
    "* __27__ - excellent score, 100% of points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: subword-nmt in /home/brayand/Storage/School/deep-learning-in-applications-homeworks/.venv/lib/python3.11/site-packages (0.3.8)\n",
      "Requirement already satisfied: mock in /home/brayand/Storage/School/deep-learning-in-applications-homeworks/.venv/lib/python3.11/site-packages (from subword-nmt) (5.1.0)\n",
      "Requirement already satisfied: tqdm in /home/brayand/Storage/School/deep-learning-in-applications-homeworks/.venv/lib/python3.11/site-packages (from subword-nmt) (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install subword-nmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def remove_tech_tokens(mystr, tokens_to_remove=['<eos>', '<sos>', '<unk>', '<pad>']):\n",
    "    return [x for x in mystr if x not in tokens_to_remove]\n",
    "\n",
    "\n",
    "def get_text(x, TRG_vocab):\n",
    "    text = [TRG_vocab.itos[token] for token in x]\n",
    "    try:\n",
    "        end_idx = text.index('<eos>')\n",
    "        text = text[:end_idx]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    text = remove_tech_tokens(text)\n",
    "    if len(text) < 1:\n",
    "        text = []\n",
    "    return text\n",
    "\n",
    "\n",
    "def generate_translation(src, trg, model, TRG_vocab):\n",
    "    model.eval()\n",
    "\n",
    "    output = model(src, trg, 0) #turn off teacher forcing\n",
    "    output = output.argmax(dim=-1).cpu().numpy()\n",
    "\n",
    "    original = get_text(list(trg[:,0].cpu().numpy()), TRG_vocab)\n",
    "    generated = get_text(list(output[1:, 0]), TRG_vocab)\n",
    "    \n",
    "    print('Original: {}'.format(' '.join(original)))\n",
    "    print('Generated: {}'.format(' '.join(generated)))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brayand/Storage/School/deep-learning-in-applications-homeworks/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_672298/3355082548.py:25: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import collections\n",
    "import utils\n",
    "import imp\n",
    "import time\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found locally. Downloading from github.\n",
      "File ‘data.txt’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Thanks to YSDA NLP course team for the data\n",
    "# (who thanks tilda and deephack teams for the data in their turn)\n",
    "\n",
    "import os\n",
    "path_do_data = '../../datasets/Machine_translation_EN_RU/data.txt'\n",
    "if not os.path.exists(path_do_data):\n",
    "    print(\"Dataset not found locally. Downloading from github.\")\n",
    "    !wget https://raw.githubusercontent.com/neychev/made_nlp_course/master/datasets/Machine_translation_EN_RU/data.txt -nc\n",
    "    path_do_data = './data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trg</th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cordelia Hotel is situated in Tbilisi, a 3-min...</td>\n",
       "      <td>Отель Cordelia расположен в Тбилиси, в 3 минут...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>At Tupirmarka Lodge you will find a 24-hour fr...</td>\n",
       "      <td>В числе удобств лоджа Tupirmarka круглосуточна...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Featuring free WiFi in all areas, Naigao Xiaow...</td>\n",
       "      <td>Апартаменты Naigao Xiaowo расположены в городе...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Each has a TV and a private bathroom with shower.</td>\n",
       "      <td>В вашем распоряжении также телевизор и собстве...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Your room comes with air conditioning and sate...</td>\n",
       "      <td>Номер оснащен кондиционером и спутниковым теле...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>The spacious air-conditioned rooms open out to...</td>\n",
       "      <td>Просторные номера с кондиционером выходят на с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Minsk Ring road is a 5-minute drive from the p...</td>\n",
       "      <td>Минская кольцевая автомобильная дорога проходи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>There is a private bathroom with a shower.</td>\n",
       "      <td>В собственной ванной комнате установлен душ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>It is a 15-minute drive from Malacca Town Cent...</td>\n",
       "      <td>За 15 минут вы доедете до центра города Малакк...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>Each room at Apartments na Podgornoy is colour...</td>\n",
       "      <td>Все апартаменты « На Подгорной» красочно оформ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     trg  \\\n",
       "0      Cordelia Hotel is situated in Tbilisi, a 3-min...   \n",
       "1      At Tupirmarka Lodge you will find a 24-hour fr...   \n",
       "2      Featuring free WiFi in all areas, Naigao Xiaow...   \n",
       "3      Each has a TV and a private bathroom with shower.   \n",
       "4      Your room comes with air conditioning and sate...   \n",
       "...                                                  ...   \n",
       "49995  The spacious air-conditioned rooms open out to...   \n",
       "49996  Minsk Ring road is a 5-minute drive from the p...   \n",
       "49997         There is a private bathroom with a shower.   \n",
       "49998  It is a 15-minute drive from Malacca Town Cent...   \n",
       "49999  Each room at Apartments na Podgornoy is colour...   \n",
       "\n",
       "                                                     src  \n",
       "0      Отель Cordelia расположен в Тбилиси, в 3 минут...  \n",
       "1      В числе удобств лоджа Tupirmarka круглосуточна...  \n",
       "2      Апартаменты Naigao Xiaowo расположены в городе...  \n",
       "3      В вашем распоряжении также телевизор и собстве...  \n",
       "4      Номер оснащен кондиционером и спутниковым теле...  \n",
       "...                                                  ...  \n",
       "49995  Просторные номера с кондиционером выходят на с...  \n",
       "49996  Минская кольцевая автомобильная дорога проходи...  \n",
       "49997       В собственной ванной комнате установлен душ.  \n",
       "49998  За 15 минут вы доедете до центра города Малакк...  \n",
       "49999  Все апартаменты « На Подгорной» красочно оформ...  \n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv(path_do_data, sep='\\t', names=['trg', 'src'])\n",
    "data_tuples = list(data_df.to_records(index=False))\n",
    "data_tuples = [(x[1], x[0]) for x in data_tuples]\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = to_map_style_dataset(data_tuples)\n",
    "\n",
    "train_len = int(len(total_data) * 0.8)\n",
    "valid_len = int(len(total_data) * 0.15)\n",
    "test_len = len(total_data) - train_len - valid_len\n",
    "\n",
    "train_data, valid_data, test_data = random_split(total_data, [train_len, valid_len, test_len], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 40000\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 2500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Approach: Basic RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_W = WordPunctTokenizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    return tokenizer_W.tokenize(text.lower())\n",
    "\n",
    "def yield_tokens(data_iter, tokenizer, language):\n",
    "    language_index = {'src': 1, 'trg': 0}[language]\n",
    "    \n",
    "    for item in data_iter:\n",
    "        yield tokenizer(item[language_index])\n",
    "        \n",
    "specials = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "init_token = '<sos>'\n",
    "eos_token = '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_src = collections.Counter()\n",
    "counter_trg = collections.Counter()\n",
    "\n",
    "for src, trg in train_data:\n",
    "    counter_src.update(tokenize(src))\n",
    "    counter_trg.update(tokenize(trg))\n",
    "\n",
    "def apply_min_freq(counter, min_freq):\n",
    "    return {word: freq for word, freq in counter.items() if freq >= min_freq}\n",
    "\n",
    "filtered_vocab_src = apply_min_freq(counter_src, 3)\n",
    "filtered_vocab_trg = apply_min_freq(counter_trg, 3)\n",
    "\n",
    "vocab_src = build_vocab_from_iterator([filtered_vocab_src], specials=specials)\n",
    "vocab_trg = build_vocab_from_iterator([filtered_vocab_trg], specials=specials)\n",
    "\n",
    "vocab_src.set_default_index(vocab_src['<unk>'])\n",
    "vocab_trg.set_default_index(vocab_trg['<unk>'])\n",
    "\n",
    "itos_src = vocab_src.get_itos()  \n",
    "itos_trg = vocab_trg.get_itos()\n",
    "\n",
    "vocab_src.itos = itos_src\n",
    "vocab_trg.itos = itos_trg\n",
    "\n",
    "def collate_batch(batch):\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    max_src_len = max(len(tokenize(src)) for src, trg in batch) + 2  # +2 for <sos> and <eos>\n",
    "    max_trg_len = max(len(tokenize(trg)) for src, trg in batch) + 2  # +2 for <sos> and <eos>\n",
    "\n",
    "    src_batch = torch.full((batch_size, max_src_len), vocab_src['<pad>'], dtype=torch.long)\n",
    "    trg_batch = torch.full((batch_size, max_trg_len), vocab_trg['<pad>'], dtype=torch.long)\n",
    "\n",
    "    for i, (src, trg) in enumerate(batch):\n",
    "        src_tokens = [vocab_src['<sos>']] + [vocab_src[token] for token in tokenize(src)] + [vocab_src['<eos>']]\n",
    "        trg_tokens = [vocab_trg['<sos>']] + [vocab_trg[token] for token in tokenize(trg)] + [vocab_trg['<eos>']]\n",
    "\n",
    "        src_batch[i, :len(src_tokens)] = torch.tensor(src_tokens, dtype=torch.long)\n",
    "        trg_batch[i, :len(trg_tokens)] = torch.tensor(trg_tokens, dtype=torch.long)\n",
    "\n",
    "    src_batch = src_batch.transpose(0, 1)\n",
    "    trg_batch = trg_batch.transpose(0, 1)\n",
    "\n",
    "    return src_batch.to(device), trg_batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source vocabulary: 9310\n",
      "Unique tokens in target vocabulary: 6711\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source vocabulary: {len(vocab_src)}\")\n",
    "print(f\"Unique tokens in target vocabulary: {len(vocab_trg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are tokens from original (RU) corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 'дерева', 'downtown', 'кататься', 'центрами', 'шах', 'margarita', 'трибхуван', 'дагомыс', 'стереосистемой']\n"
     ]
    }
   ],
   "source": [
    "print(vocab_src.itos[::1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from target (EN) corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 'palma', 'groves', 'biscuits', 'mayan', 'states', 'malagueta']\n"
     ]
    }
   ],
   "source": [
    "print(vocab_trg.itos[::1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is example from train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['в', 'распоряжении', 'гостей', 'мини', '-', 'бар', ',', 'телевизор', 'и', 'ванная', 'комната', 'с', 'ванной', 'и', 'душем', '.']\n",
      "['each', 'has', 'a', 'bathroom', 'with', 'a', 'bath', 'and', 'a', 'shower', ',', 'as', 'well', 'as', 'a', 'minibar', 'and', 'a', 'tv', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(train_data[0][0]))\n",
    "print(tokenize(train_data[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the length distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length distribution in Train data\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "src_length = map(len, [x[0] for x in train_data])\n",
    "trg_length = map(len, [x[1] for x in train_data])\n",
    "\n",
    "print('Length distribution in Train data')\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"source length\")\n",
    "plt.hist(list(src_length), bins=20);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"translation length\")\n",
    "plt.hist(list(trg_length), bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length distribution in Test data\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "src_length = map(len, [x[0] for x in test_data])\n",
    "trg_length = map(len, [x[1] for x in test_data])\n",
    "\n",
    "print('Length distribution in Test data')\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"source length\")\n",
    "plt.hist(list(src_length), bins=20);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"translation length\")\n",
    "plt.hist(list(trg_length), bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "__Here comes simple pipeline of NMT model learning. It almost copies the week03 practice__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_network.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_network.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_dim, embedding_dim=emb_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hid_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "\n",
    "        embedded = self.embedding(src)\n",
    "\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=output_dim, embedding_dim=emb_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hid_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(in_features=hid_dim, out_features=output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.out(output.squeeze(0))\n",
    "\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert (\n",
    "            encoder.hid_dim == decoder.hid_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch, generator=torch.Generator().manual_seed(42))\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch, generator=torch.Generator().manual_seed(42))\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch, generator=torch.Generator().manual_seed(42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  2,   2,   2,  ...,   2,   2,   2],\n",
      "        [252,   5,  34,  ...,   5,  67,  21],\n",
      "        [253,  71,  13,  ..., 119, 207,  25],\n",
      "        ...,\n",
      "        [  1,   1,   1,  ...,   1,   1,   1],\n",
      "        [  1,   1,   1,  ...,   1,   1,   1],\n",
      "        [  1,   1,   1,  ...,   1,   1,   1]]), tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
      "        [ 241,  425, 1869,  ...,   92,   28,   30],\n",
      "        [ 759,   12,    0,  ...,  425,  215,   27],\n",
      "        ...,\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1]]))\n",
      "torch.Size([65, 128]) torch.Size([60, 128])\n"
     ]
    }
   ],
   "source": [
    "for x in train_loader:\n",
    "    break\n",
    "print(x)\n",
    "print(x[0].shape, x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_network\n",
    "Encoder = my_network.Encoder\n",
    "Decoder = my_network.Decoder\n",
    "Seq2Seq = my_network.Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_src)\n",
    "OUTPUT_DIM = len(vocab_trg)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "# dont forget to put the model to the right device\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(9310, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(6711, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (out): Linear(in_features=512, out_features=6711, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    # <YOUR CODE HERE>\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 14,900,535 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = vocab_trg['<pad>']\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip, train_history=None, valid_history=None):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    history = []\n",
    "    for i, batch in tqdm.tqdm(enumerate(iterator)):\n",
    "        \n",
    "        src = batch[0]\n",
    "        trg = batch[1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg sent len, batch size]\n",
    "        #output = [trg sent len, batch size, output dim]\n",
    "        \n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        # trg[1:].reshape(-1)\n",
    "        \n",
    "        #trg = [(trg sent len - 1) * batch size]\n",
    "        #output = [(trg sent len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Let's clip the gradient\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        history.append(loss.cpu().data.numpy())\n",
    "        if (i+1)%10==0:\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n",
    "\n",
    "            clear_output(True)\n",
    "            ax[0].plot(history, label='train loss')\n",
    "            ax[0].set_xlabel('Batch')\n",
    "            ax[0].set_title('Train loss')\n",
    "            if train_history is not None:\n",
    "                ax[1].plot(train_history, label='general train history')\n",
    "                ax[1].set_xlabel('Epoch')\n",
    "            if valid_history is not None:\n",
    "                ax[1].plot(valid_history, label='general valid history')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.show()\n",
    "\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch[0]\n",
    "            trg = batch[1]\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg sent len, batch size]\n",
    "            #output = [trg sent len, batch size, output dim]\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            #trg = [(trg sent len - 1) * batch size]\n",
    "            #output = [(trg sent len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = []\n",
    "valid_history = []\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:06, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "Cell \u001b[0;32mIn[24], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip, train_history, valid_history)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# trg[1:].reshape(-1)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#trg = [(trg sent len - 1) * batch size]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#output = [(trg sent len - 1) * batch size, output dim]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, trg)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Let's clip the gradient\u001b[39;00m\n\u001b[1;32m     30\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n",
      "File \u001b[0;32m~/Storage/School/deep-learning-in-applications-homeworks/.venv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Storage/School/deep-learning-in-applications-homeworks/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP, train_history, valid_history)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    train_history.append(train_loss)\n",
    "    valid_history.append(valid_loss)\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load trained model\n",
    "\n",
    "model.load_state_dict(torch.load('tut1-model.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(utils)\n",
    "generate_translation = utils.generate_translation\n",
    "remove_tech_tokens = utils.remove_tech_tokens\n",
    "get_text = utils.get_text\n",
    "flatten = utils.flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: each room here will provide you with air conditioning .\n",
      "Generated: all rooms are air conditioned .\n",
      "\n",
      "Original: take advantage of the large , free car park .\n",
      "Generated: free parking is available on site .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in [1,2]:\n",
    "    src = batch[0][:, idx:idx+1]\n",
    "    trg = batch[1][:, idx:idx+1]\n",
    "    generate_translation(src, trg, model, vocab_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     \"\"\" Estimates corpora-level BLEU score of model's translations given inp and reference out \"\"\"\n",
    "#     translations, _ = model.translate_lines(inp_lines, **flags)\n",
    "#     # Note: if you experience out-of-memory error, split input lines into batches and translate separately\n",
    "#     return corpus_bleu([[ref] for ref in out_lines], translations) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:19,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "original_text = []\n",
    "generated_text = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    for i, batch in tqdm.tqdm(enumerate(test_loader)):\n",
    "\n",
    "        src = batch[0]\n",
    "        trg = batch[1]\n",
    "\n",
    "        output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #output = [trg sent len, batch size, output dim]\n",
    "\n",
    "        output = output.argmax(dim=-1)\n",
    "        \n",
    "        original_text.extend([get_text(x, vocab_trg) for x in trg.cpu().numpy().T])\n",
    "        generated_text.extend([get_text(x, vocab_trg) for x in output[1:].detach().cpu().numpy().T])\n",
    "\n",
    "# original_text = flatten(original_text)\n",
    "# generated_text = flatten(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.568261612092304"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bleu([[text] for text in original_text], generated_text) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 3: fine-tunning BART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_name = \"facebook/bart-base\"\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(bart_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    src_texts = [item[0] for item in batch]  \n",
    "    trg_texts = [item[1] for item in batch]\n",
    "\n",
    "    tokenizer = bart_tokenizer(\n",
    "        text=src_texts,\n",
    "        text_target=trg_texts,\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    labels = tokenizer[\"labels\"]\n",
    "    labels[labels == bart_tokenizer.pad_token_id] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenizer[\"input_ids\"],\n",
    "        \"attention_mask\": tokenizer[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch, generator=torch.Generator().manual_seed(42))\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch, generator=torch.Generator().manual_seed(42))\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch, generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device, train_history=None, valid_history=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    history = []\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        history.append(loss.cpu().data.numpy())\n",
    "        if (i+1)%10==0:\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n",
    "\n",
    "            clear_output(True)\n",
    "            ax[0].plot(history, label='train loss')\n",
    "            ax[0].set_xlabel('Batch')\n",
    "            ax[0].set_title('Train loss')\n",
    "            if train_history is not None:\n",
    "                ax[1].plot(train_history, label='general train history')\n",
    "                ax[1].set_xlabel('Epoch')\n",
    "            if valid_history is not None:\n",
    "                ax[1].plot(valid_history, label='general valid history')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "        i += 1\n",
    "\n",
    "    print(f\"Average training loss: {total_loss / len(data_loader)}\")\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    print(f\"Average validation loss: {total_loss / len(data_loader)}\")\n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139420416"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart = BartForConditionalGeneration.from_pretrained(bart_name).to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(bart):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 4/938 [00:28<1:52:11,  7.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# train_loss = train_epoch(bart, train_loader, optimizer, device)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_loss \u001b[38;5;241m<\u001b[39m best_valid_loss:\n\u001b[1;32m     14\u001b[0m     best_valid_loss \u001b[38;5;241m=\u001b[39m valid_loss\n",
      "Cell \u001b[0;32mIn[24], line 34\u001b[0m, in \u001b[0;36meval_model\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 34\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     38\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Storage/School/deep-learning-in-applications-homeworks/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Storage/School/deep-learning-in-applications-homeworks/.venv/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:1728\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1724\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1725\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1726\u001b[0m         )\n\u001b[0;32m-> 1728\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1746\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1747\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Storage/School/deep-learning-in-applications-homeworks/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Storage/School/deep-learning-in-applications-homeworks/.venv/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:1596\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1593\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1596\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "File \u001b[0;32m~/Storage/School/deep-learning-in-applications-homeworks/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Storage/School/deep-learning-in-applications-homeworks/.venv/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:1204\u001b[0m, in \u001b[0;36mBartEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1197\u001b[0m             encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1198\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             output_attentions,\n\u001b[1;32m   1202\u001b[0m         )\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1204\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1211\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/Storage/School/deep-learning-in-applications-homeworks/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Storage/School/deep-learning-in-applications-homeworks/.venv/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:659\u001b[0m, in \u001b[0;36mBartEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;124;03m        returned tensors for more detail.\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    658\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 659\u001b[0m hidden_states, attn_weights, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    666\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/Storage/School/deep-learning-in-applications-homeworks/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Storage/School/deep-learning-in-applications-homeworks/.venv/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:234\u001b[0m, in \u001b[0;36mBartAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    231\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mproj_shape)\n\u001b[1;32m    233\u001b[0m src_len \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 234\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_weights\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention weights should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mtgt_len,\u001b[38;5;250m \u001b[39msrc_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_weights\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "optimizer = optim.AdamW(bart.parameters(), lr=5e-6)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "train_history = []\n",
    "valid_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    train_loss = train_epoch(bart, train_loader, optimizer, device, train_history, valid_history)\n",
    "    valid_loss = eval_model(bart, valid_loader, device)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(bart.state_dict(), 'bart-model.pt')\n",
    "\n",
    "    train_history.append(train_loss)\n",
    "    valid_history.append(valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.load_state_dict(torch.load('bart-model.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "157it [27:00, 10.32s/it]\n"
     ]
    }
   ],
   "source": [
    "original_text = []\n",
    "generated_text = []\n",
    "\n",
    "def translate(batch):\n",
    "\n",
    "    src_ids = batch[\"input_ids\"].to(device)\n",
    "    src_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    outputs = bart.generate(\n",
    "        src_ids,\n",
    "        attention_mask=src_mask,\n",
    "        num_beams=5,\n",
    "        max_length=100,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    src_texts = [\n",
    "        bart_tokenizer.decode(s, skip_special_tokens=True) for s in src_ids\n",
    "    ]\n",
    "\n",
    "    translated_texts = [\n",
    "        bart_tokenizer.decode(o, skip_special_tokens=True) for o in outputs\n",
    "    ]\n",
    "\n",
    "    trg_texts = [\n",
    "        bart_tokenizer.decode(t[t != -100], skip_special_tokens=True)\n",
    "        for t in labels\n",
    "    ]\n",
    "\n",
    "    return src_texts, trg_texts, translated_texts\n",
    "\n",
    "\n",
    "bart.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in tqdm(enumerate(test_loader)):\n",
    "\n",
    "        src_texts, trg_texts, translated_texts = translate(batch)\n",
    "\n",
    "        original_text.extend(trg_texts)\n",
    "        generated_text.extend(translated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It offers well-equipped rooms and a daily breakfast buffet.', 'Each room here will provide you with air conditioning.', 'Take advantage of the large, free car park.', 'Air conditioned rooms are provided with a flat-screen TV, seating area and minibar.', 'Complejo Alcaidia De Magina is located in the charming Andalusian hillside town of Cambil.']\n",
      "['It offers spacious rooms and a daily buffet breakfast.', 'Each room here will provide you with air conditioning.', 'Free parking is available on site.', 'The air-conditioned rooms feature a flat-screen TV, seating area and minibar.', 'Hotel Complejo Alcaidia De Magina is located in the centre of Cáceres.']\n"
     ]
    }
   ],
   "source": [
    "print(original_text[:5])\n",
    "print(generated_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.708372672512546"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bleu([[text] for text in original_text], generated_text) * 100"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
